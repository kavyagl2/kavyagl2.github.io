
## DBPedia @ GSoC'24 - Weekly Updates

##### Grateful to DBPedia for choosing me to work on an excellent open-source project **Multilingual Data-to-Text Conversion** with Diego, Bruno and Anand!

## Let's get started!

### Community Bonding Period (28-May-2024)
This was essentially the most fantastic day as I got to interact with fellow contributors and all the mentor. We had a discussion on how things are setup at DBPedia along with the other cool projects that contibutors are working on. We also got a chance to introduce ourselves which made me comfortable with my fellow contributors. In the end, we had so many photos together and ended meetup on **HIGH JOSH!**

#### HAPPPPIIIEEE FACES!
![alt text][logo]

[logo]: https://github.com/kavyagl2/Google-Summer-of-Code-24/assets/79780905/d15c6fba-0ecc-4e9e-8967-bf21fc5d3961 "DBPedia GSoc'24 Meetup"

## WEEK - 1 (10 June 2024 to 17 June 2024)
##### A week for research! 
This week, I mostly spent my time doing research on NABU which is a **Multilingual Graph Based Neural RDF Verbalizer**. Going forward, what I did in this week we will discuss what NABU is, the architecture of NABU, and how is RDF data being utilized to verbalize German, Russian and English. There is extensive use of WebNLG data which come from past GSoc events and competitions. 

### What is NABU?
NABU is a multilingual graph-based neural model that verbalizes RDF data to German, Russian and English. NABU is based on encoder-decoder architecture, incorporating structural information of RDF triples using encoder which is inspired by Grpah Attention Network and a Transformer Decoder. NABU is based on the fact that knowledge graphs are language agnostic and hence can be used on the encoder side to generate multilingual text. NABU relies on the use of a reification strategy for modelling the graph structure of RDF input. 
NABU is evaluated on the standard benchmarking WebNLG datasets in three settings: monolingual, multilingual and bilingual. 

**1. Monolingual:** NABU was compared with state-of-the-art English approaches and also performed experiments on Russian and German. 

**2. Bilingual:** Model was trained and evaluated in English - German and on English - Russian. 

**3. Multilingual:** NABU was compared with the multilingual Transformer model in English, German and Russian. 

### NABU's Approach: 
**Input -**  RDF Graph.

**Output -** Output text in the desired language reflecting input's meaning. 

**RDF Triples -** Subject ---> Value. Triples are statements about things using URIs and Literal values. 

**RDF Format -** 
* N-triples.

* Turtle.

* RDF/XMI.

* RDF JSON.

* RDFa.

* JSON-LD.

**MAIN IDEA-**

Given the KGs are language-agnostic and represent facts often extracted from text, we can regard the facts (i.e. RDF Triples) as sentences and train a model to translate the facts from a language-agnostic graph representation to several languages.

**APPROACH**

Graph-based neural networks have been successfully used for parsing and generating natural-language sentences from RDF knowledge graphs. While Graph Attention Networks (GAT) have helped mitigate the loss of node information, there was the issue of parameter explosion, which can be exacerbated by the size of the graph structure. To address this parameter explosion problem, a reification strategy was utilized to make slight modifications to the encoding of the RDF graph.

#### Before moving forward, let us have a glance at what is Transformer and Encoder as discussed in the paper. 

##### **What are TRANSFORMERS?**

Transformer-based models consist of an encoder and a decoder. The encoder reads the input sequence \( x = (x_1, ..., x_n) \) and the decoder predicts the target sequence \( y = (y_1, ..., y_n) \), interacting via a soft-attention mechanism.

_**Notations from Tang et al.:**_
- \( m \): word embedding size
- \( n \): number of hidden units
- \( K \): vocabulary size
- ![hidden state](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/a5c8e173-525f-4e31-b50a-c981c34844d7) : hidden state at step \( i \) of layer \( l \)
- ![word embedding matrix](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/0d63d36f-35a3-40f4-81ac-a08a9e3414d0) : word embedding matrix
- ![weight matrices](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/6bdd7908-d3ed-44f0-b984-878cd2e9c4f2) : weight matrices
- ![embedding of x_i](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/c01c3170-ec1a-485e-9175-cd17284383b1) : embedding of \( x_i \)
- ![psoitional embedding](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/cc7dfff8-3192-40ee-ab86-34c2ed84294d) : positional embedding at position \( i \)

**Key features:**
- Self-attention networks: direct connections between tokens with path length 1.
- No recurrence: positional encoding for input and output.
- Multi-head attention: more complex than single-head attention in RNNs.

**Hidden state calculation:**
- First layer: ![First Layer](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/0f1e79b6-ad03-4039-af98-478ac8778913)

- Decoder: ![Decoder](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/e554f884-36a8-497b-b727-73d30195c27c)

##### **What is GRAPH ATTENTION NETWORK?**
Kipf and Welling introduced Graph Convolutional Networks (GCN), which extend CNN convolution operations to graph structures. Each GCN layer uses a weight matrix 
ğ‘Š to transform node feature vectors from a low-dimensional to a high-dimensional space, preserving the graph structure.
For a graph with ğ‘§ nodes and features ![image](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/47c877f6-c186-4ae0-9654-9cacf62fab40) a GCN layer computes new features ![image](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/3b2b6c70-cce4-42fb-880a-1f994293de4d) : 

![image](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/17bd20ef-8ed2-4b5e-8063-ee94bc30348c)

However, GCNs don't account for the varying importance of nodes. Velickovic et al. introduced Graph Attention Networks (GAT), which use dynamic attention coefficients. A GAT layer calculates a score e_ij for each node pair:![image](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/70c3b9c5-8471-40a0-b987-3c0ba154f711)

The scores are normalized using softmax:
![image](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/848a1ec0-acb8-4924-be6a-36a92c4716b8)

This allows GATs to dynamically weigh the importance of different nodes.

### NABU's Architecture
![NABU Architecture](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/47220ef7-f9cf-4dfe-aced-e1862c1bae87)

Input Data [**RDF Data**] ------> **Encoder GAT** ------> **Decoder Transformer** ------> **OUTPUT** [English, Russian, German]

**1) Reification**

Reification helps in two ways: it generates a hidden state for each relation, and it models an arbitrary number of predicates efficiently.
![Reification](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/a26f6786-a41e-4ba4-9bc6-f195ee850764)

RDF triples are represented as a graph where subjects and objects are nodes, and predicates are labeled edges. For example, <Albert_Einstein, birthPlace, Germany> is a sub-graph in DBpedia with Albert_Einstein and Germany as nodes, and birthPlace as the edge. 

In Graph Attention Networks (GAT), edges are encoded as parameters, which can lead to parameter explosion as noted by Beck et al.

To solve this, we use reification. This strategy maps relations to nodes and creates two new binary relations for each RDF triple: one for the subject-predicate relationship (A0) and one for the predicate-object relationship (A1). Thus, <Albert_Einstein, birthPlace, Germany> becomes <Albert_Einstein, A0, birthPlace> and <birthPlace, A1, Germany>.

**2) Encoder**

The reified graph is input to the GAT, which applies a self-attention mechanism to compute the importance of each node. **GAT Encoder** represents nodes in a high-dimensional vector space and considers the representations of neighbouring nodes. NABU uses a special token in the encoder to determine the target language, similar to recent multilingual Neural Machine Translation (NMT) models. 

**- Forward Pass in NABU:**
  **Inputs:** Four dense vectors
  - Node vector H: ![NODE VECTOR](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/602985eb-f704-44bd-b395-ed0bc0891109)
  - Source vector ğ‘†: ![Source Vector](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/4c77e9c2-5c02-47f0-a61f-7d69ef682226)
  - Destination vector ğ·: ![Destination Vector](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/44e7e86f-5fe7-42e9-9489-3de09e12c50e)
  - Label vector ğ¿: ![Label Vector](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/4ce9586e-d5d5-4e70-8289-7822a78459e7)

  **Edge Vector** Concatenate source ğ‘† and destination ğ· vectors, pass through a dense layer to encode into a vector of the same shape as the label vector and then Edge vector ğ¸ is formed as ![Edge Vector](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/98e2af62-5daa-4da0-b3fc-6d41800a12a1)

  **Input to Encoder** combine edge vector E, node vector H, and label vector L:![Input to Encoder](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/64318b9e-4167-4950-af6d-a6124c856687), Where ğœ‚ is the number of heads in the multi-head attention layer.



  
































