## Welcome to Kavya Agrawal's GSoC'24 Page
##### Grateful to DBPedia for choosing me to work on an excellent open-source project **Multilingual Data-to-Text Conversion** with Diego, Bruno and Anand!

## Let's get started!

### Community Bonding Period (28-May-2024)
This was essentially the most fantastic day as I got to interact with fellow contributors and all the mentor. We had a discussion on how things are setup at DBPedia along with the other cool projects that contibutors are working on. We also got a chance to introduce ourselves which made me comfortable with my fellow contributors. In the end, we had so many photos together and ended meetup on **HIGH JOSH!**

#### HAPPPPIIIEEE FACES!
![alt text][logo]

[logo]: https://github.com/kavyagl2/Google-Summer-of-Code-24/assets/79780905/d15c6fba-0ecc-4e9e-8967-bf21fc5d3961 "DBPedia GSoc'24 Meetup"

## WEEK - 1 (10 June 2024 to 17 June 2024)
##### A week for research! 
This week, I mostly spent my time doing research on NABU which is a **Multilingual Graph Based Neural RDF Verbalizer**. Going forward, what I did in this week we will discuss what NABU is, the architecture of NABU, and how is RDF data being utilized to verbalize German, Russian and English. There is extensive use of WebNLG data which come from past GSoc events and competitions. 

### What is NABU?
NABU is a multilingual graph based neural model that verbalizes RDF data to German, Russian and English. NABU is based on encoder-decoder architecture, incorporating structural information of RDF triples using encoder which is inspired by Grpah Attention Network and a Transformer Decoder. NABU is based on the fact that knowledge graphs are language agnostic and hence can be used on the encoder side to generate multilingual text. NABU relies on the use of a reification strategy for modelling the graph structure of RDF input. 
NABU is evaluated on the standard benchmarking WebNLG datasets in three settings: monolingual, multilingual and bilingual. 

**1. Monolingual:** NABU was compared with state-of-the-art English approaches and also performed experiments on Russian and German. 

**2. Bilingual:** Model was trained and evaluated in English - German and on English - Russian. 

**3. Multilingual:** NABU was compared with the multilingual Transformer model in English, German and Russian. 

### NABU's Approach: 
**Input -**  RDF Graph.

**Output -** Output text in the desired language reflecting input's meaning. 

**RDF Triples -** Subject ---> Value. Triples are statements about things using URIs and Literal values. 

**RDF Format -** 
* N-triples.

* Turtle.

* RDF/XMI.

* RDF JSON.

* RDFa.

* JSON-LD.

**MAIN IDEA-**

Given the KGs are language-agnostic and represent facts often extracted from text, we can regard the facts (i.e. RDF Triples) as sentences and train a model to translate the facts from a language-agnostic graph representation to several languages.

**APPROACH**

Graph-based neural networks have been successfully used for parsing and generating natural-language sentences from RDF knowledge graphs. While Graph Attention Networks (GAT) have helped mitigate the loss of node information, there was the issue of parameter explosion, which can be exacerbated by the size of the graph structure. To address this parameter explosion problem, a reification strategy was utilized to make slight modifications to the encoding of the RDF graph.

#### Before moving forward, let us have a glance at what is Transformer and Encoder as discussed in the paper. 

##### **What are TRANSFORMERS?**

Transformer-based models consist of an encoder and a decoder. The encoder reads the input sequence \( x = (x_1, ..., x_n) \) and the decoder predicts the target sequence \( y = (y_1, ..., y_n) \), interacting via a soft-attention mechanism.

_**Notations from Tang et al.:**_
- \( m \): word embedding size
- \( n \): number of hidden units
- \( K \): vocabulary size
- ![hidden state](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/a5c8e173-525f-4e31-b50a-c981c34844d7) : hidden state at step \( i \) of layer \( l \)
- ![word embedding matrix](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/0d63d36f-35a3-40f4-81ac-a08a9e3414d0) : word embedding matrix
- ![weight matrices](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/6bdd7908-d3ed-44f0-b984-878cd2e9c4f2) : weight matrices
- ![embedding of x_i](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/c01c3170-ec1a-485e-9175-cd17284383b1) : embedding of \( x_i \)
- ![psoitional embedding](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/cc7dfff8-3192-40ee-ab86-34c2ed84294d) : positional embedding at position \( i \)

**Key features:**
- Self-attention networks: direct connections between tokens with path length 1.
- No recurrence: positional encoding for input and output.
- Multi-head attention: more complex than single-head attention in RNNs.

**Hidden state calculation:**
- First layer: ![First Layer](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/0f1e79b6-ad03-4039-af98-478ac8778913)

- Decoder: ![Decoder](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/e554f884-36a8-497b-b727-73d30195c27c)

##### **What is GRAPH ATTENTION NETWORK?**






















