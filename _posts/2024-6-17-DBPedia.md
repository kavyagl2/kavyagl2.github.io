
## DBPedia @ GSoC'24 - Weekly Updates

##### Grateful to DBPedia for choosing me to work on an excellent open-source project **Multilingual Data-to-Text Conversion** with Diego, Bruno and Anand!

## Let's get started!

### Community Bonding Period (28-May-2024)
This was essentially the most fantastic day as I got to interact with fellow contributors and all the mentor. We had a discussion on how things are setup at DBPedia along with the other cool projects that contibutors are working on. We also got a chance to introduce ourselves which made me comfortable with my fellow contributors. In the end, we had so many photos together and ended meetup on **HIGH JOSH!**

#### HAPPPPIIIEEE FACES!
![alt text][logo]

[logo]: https://github.com/kavyagl2/Google-Summer-of-Code-24/assets/79780905/d15c6fba-0ecc-4e9e-8967-bf21fc5d3961 "DBPedia GSoc'24 Meetup"

## WEEK - 1 (10 June 2024 to 17 June 2024)
##### A week for research! 
This week, I mostly spent my time doing research on NABU which is a **Multilingual Graph Based Neural RDF Verbalizer**. Going forward, what I did in this week we will discuss what NABU is, the architecture of NABU, and how is RDF data being utilized to verbalize German, Russian and English. There is extensive use of WebNLG data which come from past GSoc events and competitions. 

### What is NABU?
NABU is a multilingual graph-based neural model that verbalizes RDF data to German, Russian and English. NABU is based on encoder-decoder architecture, incorporating structural information of RDF triples using encoder which is inspired by Grpah Attention Network and a Transformer Decoder. NABU is based on the fact that knowledge graphs are language agnostic and hence can be used on the encoder side to generate multilingual text. NABU relies on the use of a reification strategy for modelling the graph structure of RDF input. 
NABU is evaluated on the standard benchmarking WebNLG datasets in three settings: monolingual, multilingual and bilingual. 

**1. Monolingual:** NABU was compared with state-of-the-art English approaches and also performed experiments on Russian and German. 

**2. Bilingual:** Model was trained and evaluated in English - German and on English - Russian. 

**3. Multilingual:** NABU was compared with the multilingual Transformer model in English, German and Russian. 

### NABU's Approach: 
**Input -**  RDF Graph.

**Output -** Output text in the desired language reflecting input's meaning. 

**RDF Triples -** Subject ---> Value. Triples are statements about things using URIs and Literal values. 

**RDF Format -** 
* N-triples.

* Turtle.

* RDF/XMI.

* RDF JSON.

* RDFa.

* JSON-LD.

**MAIN IDEA-**

Given the KGs are language-agnostic and represent facts often extracted from text, we can regard the facts (i.e. RDF Triples) as sentences and train a model to translate the facts from a language-agnostic graph representation to several languages.

**APPROACH**

Graph-based neural networks have been successfully used for parsing and generating natural-language sentences from RDF knowledge graphs. While Graph Attention Networks (GAT) have helped mitigate the loss of node information, there was the issue of parameter explosion, which can be exacerbated by the size of the graph structure. To address this parameter explosion problem, a reification strategy was utilized to make slight modifications to the encoding of the RDF graph.

#### Before moving forward, let us have a glance at what is Transformer and Encoder as discussed in the paper. 

##### **What are TRANSFORMERS?**

Transformer-based models consist of an encoder and a decoder. The encoder reads the input sequence \( x = (x_1, ..., x_n) \) and the decoder predicts the target sequence \( y = (y_1, ..., y_n) \), interacting via a soft-attention mechanism.

_**Notations from Tang et al.:**_
- \( m \): word embedding size
- \( n \): number of hidden units
- \( K \): vocabulary size
- ![hidden state](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/a5c8e173-525f-4e31-b50a-c981c34844d7) : hidden state at step \( i \) of layer \( l \)
- ![word embedding matrix](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/0d63d36f-35a3-40f4-81ac-a08a9e3414d0) : word embedding matrix
- ![weight matrices](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/6bdd7908-d3ed-44f0-b984-878cd2e9c4f2) : weight matrices
- ![embedding of x_i](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/c01c3170-ec1a-485e-9175-cd17284383b1) : embedding of \( x_i \)
- ![psoitional embedding](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/cc7dfff8-3192-40ee-ab86-34c2ed84294d) : positional embedding at position \( i \)

**Key features:**
- Self-attention networks: direct connections between tokens with path length 1.
- No recurrence: positional encoding for input and output.
- Multi-head attention: more complex than single-head attention in RNNs.

**Hidden state calculation:**
- First layer: ![First Layer](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/0f1e79b6-ad03-4039-af98-478ac8778913)

- Decoder: ![Decoder](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/e554f884-36a8-497b-b727-73d30195c27c)

##### **What is GRAPH ATTENTION NETWORK?**
Kipf and Welling introduced Graph Convolutional Networks (GCN), which extend CNN convolution operations to graph structures. Each GCN layer uses a weight matrix 
ùëä to transform node feature vectors from a low-dimensional to a high-dimensional space, preserving the graph structure.
For a graph with ùëß nodes and features ![image](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/47c877f6-c186-4ae0-9654-9cacf62fab40) a GCN layer computes new features ![image](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/3b2b6c70-cce4-42fb-880a-1f994293de4d) : 

![image](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/17bd20ef-8ed2-4b5e-8063-ee94bc30348c)

However, GCNs don't account for the varying importance of nodes. Velickovic et al. introduced Graph Attention Networks (GAT), which use dynamic attention coefficients. A GAT layer calculates a score e_ij for each node pair:![image](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/70c3b9c5-8471-40a0-b987-3c0ba154f711)

The scores are normalized using softmax:
![image](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/848a1ec0-acb8-4924-be6a-36a92c4716b8)

This allows GATs to dynamically weigh the importance of different nodes.

### NABU's Architecture
![NABU Architecture](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/47220ef7-f9cf-4dfe-aced-e1862c1bae87)

Input Data [**RDF Data**] ------> **Encoder GAT** ------> **Decoder Transformer** ------> **OUTPUT** [English, Russian, German]

**1) Reification**

Reification helps in two ways: it generates a hidden state for each relation, and it models an arbitrary number of predicates efficiently.
![Reification](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/a26f6786-a41e-4ba4-9bc6-f195ee850764)

RDF triples are represented as a graph where subjects and objects are nodes, and predicates are labeled edges. For example, <Albert_Einstein, birthPlace, Germany> is a sub-graph in DBpedia with Albert_Einstein and Germany as nodes, and birthPlace as the edge. 

In Graph Attention Networks (GAT), edges are encoded as parameters, which can lead to parameter explosion as noted by Beck et al.

To solve this, we use reification. This strategy maps relations to nodes and creates two new binary relations for each RDF triple: one for the subject-predicate relationship (A0) and one for the predicate-object relationship (A1). Thus, <Albert_Einstein, birthPlace, Germany> becomes <Albert_Einstein, A0, birthPlace> and <birthPlace, A1, Germany>.

**2) Encoder**

The reified graph is input to the GAT, which applies a self-attention mechanism to compute the importance of each node. **GAT Encoder** represents nodes in a high-dimensional vector space and considers the representations of neighbouring nodes. NABU uses a special token in the encoder to determine the target language, similar to recent multilingual Neural Machine Translation (NMT) models. 

**- Forward Pass in NABU:**

  **Inputs:** Four dense vectors
  - Node vector H: ![NODE VECTOR](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/602985eb-f704-44bd-b395-ed0bc0891109)
  - Source vector ùëÜ: ![Source Vector](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/4c77e9c2-5c02-47f0-a61f-7d69ef682226)
  - Destination vector ùê∑: ![Destination Vector](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/44e7e86f-5fe7-42e9-9489-3de09e12c50e)
  - Label vector ùêø: ![Label Vector](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/4ce9586e-d5d5-4e70-8289-7822a78459e7)

  **Edge Vector** Concatenate source ùëÜ and destination ùê∑ vectors, pass through a dense layer to encode into a vector of the same shape as the label vector and then Edge vector ùê∏ is formed as ![Edge Vector](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/98e2af62-5daa-4da0-b3fc-6d41800a12a1)

  **Input to Encoder** combine edge vector E, node vector H, and label vector L:![Input to Encoder](https://github.com/kavyagl2/kavyagl2.github.io/assets/79780905/64318b9e-4167-4950-af6d-a6124c856687), Where ùúÇ is the number of heads in the multi-head attention layer.

**3) Decoder**
The decoder uses the standard Transformer decoder architecture, considering the intermediate representation generated by the encoder. The decoder outputs a probability distribution over the target language's vocabulary.
To handle multilingualism and out-of-vocabulary words, an unsupervised tokenizer implements Byte Pair Encoding (BPE) and a unigram language model. Finally, a beam search is applied to select the most likely words in the output sentence.

### TASKS at HAND

- **Three Evaluation Tasks**:

  1. **Monolingual**:
     - Train separate models for each language: English, German, and Russian.
     - Each model receives RDF triples from its respective DBpedia version.
     - Evaluation is performed on each WebNLG language-specific dataset.
  
  2. **Bilingual**:
     - Two sets of models:
       - **English-German Model**: Receives RDF triples from English and German DBpedia versions, and outputs text in both languages.
       - **English-Russian Model**: Receives RDF triples from English and Russian DBpedia versions, and outputs text in both languages.
  
  3. **Multilingual**:
     - Train a single model to handle triples from English, German, and Russian DBpedia versions.
     - The model outputs text in all three languages.
     - Inputs include WebNLG triples with resources from the English, German, and Russian DBpedia KGs, using sameAs relations for completeness.
    
### Evaluation Metrics 

- **BLEU**:
  - Uses modified precision for comparing MT output with reference translations.
  - Measures n-gram similarity (n=1 to 4) at the word level.
  - Applies a brevity penalty based on output length compared to the reference.

- **METEOR**:
  - Addresses semantic weaknesses of BLEU.
  - Considers stemming, paraphrasing, and exact word matching.
  - Incorporates synonymy overlap via WordNet synsets.

- **CHRF++**:
  - Uses character n-gram precision and recall (F-score).
  - Correlates well with human rankings of MT outputs.
  - Language- and tokenization-independent.

### FUTURE SCOPE OF WORK

- First, there is a need to enhance the generation of entities to fix typos and inaccuracies produced by NABU. Additionally, increasing the diversity of generated synonyms is crucial to improving METEOR scores, as this metric penalizes low synonym variation. Another important area is refining evaluation metrics to better capture the quality of generated text, considering both precision and semantic accuracy.

- Furthermore, there were issues related to incorrect verbalization of similar predicates (edges) like "dbo: artist" and "dbo: producer", which lowered NABU scores across all languages. This issue particularly affected NABU's text generation in the Artist domain. The problem arises from RDF triples containing both "dbo: artist" and "dbo: producer" relations as predicates. These predicates share the same domain and range, resulting in both being verbalized as "artist" due to their similar vector representations in embeddings. To resolve this issue, one potential plan is to employ a more appropriate embedding model that can effectively distinguish between these semantically similar predicates.

- Reification strategy impacted Discourse Ordering, particularly evident when handling RDF triples with identical predicates for different subjects, such as "Albert_Einstein dbo: birthplace Germany" and "Michael_Jackson dbo: birthplace USA". NABUGAT-Trans sometimes verbalized these triples inaccurately, merging subjects and locations incorrectly due to identical handling on the encoder side. To address this limitation, future research can explore new approaches to improve both the structuring and ordering steps in text generation.

- There are challenges with word inflections in German, particularly with possessives during verbalization. For example, the phrase "Elliot See's Besatzung war ein Testpilot." had a misplaced apostrophe ('s). However, this issue did not occur in sentences like "Bill Oddies Tochter ist Kate Hardie", where the possessive was correctly constructed. Similarly, the preposition "von" (en: of) posed difficulties. For instance, "Texas_University" was incorrectly translated as "Universit√§t von Texas" instead of "Universit√§t Texas". These challenges are well-documented in Machine Translation (MT) from English to German. In future research, we can plan to explore these phenomena further to enhance the accuracy of German text generation.
   
- There is a strong emphasis on enhancing NABU's performance by exploring alternative graph-based neural architectures and different reification approaches. Additionally, we are advised to investigate strategies for handling relation similarities by integrating language models and exploring new evaluation metrics. Research can also be extended to testing current methodology in low-resource scenarios and across different Knowledge Graphs (KGs).



  
































